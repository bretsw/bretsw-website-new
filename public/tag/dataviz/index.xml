<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dataviz | Digital space of K. Bret Staudt Willet</title>
    <link>http://bretsw.com/tag/dataviz/</link>
      <atom:link href="http://bretsw.com/tag/dataviz/index.xml" rel="self" type="application/rss+xml" />
    <description>dataviz</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 09 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://bretsw.com/media/icon_hu327cdc32eb61737bbb22c3db2388dbf1_47180_512x512_fill_lanczos_center_3.png</url>
      <title>dataviz</title>
      <link>http://bretsw.com/tag/dataviz/</link>
    </image>
    
    <item>
      <title>Visualizing Inter-rater Reliability</title>
      <link>http://bretsw.com/post/irr-visual/</link>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      <guid>http://bretsw.com/post/irr-visual/</guid>
      <description>&lt;h2 id=&#34;background-on-reporting-inter-rater-reliability&#34;&gt;Background on Reporting Inter-rater Reliability&lt;/h2&gt;
&lt;p&gt;Qualitative studies often report inter-reliability (IRR) scores as a measure of the trustworthiness of coding, or an assurance to readers that they might follow the researcher&amp;rsquo;s codebook and expect to find similar results. &lt;em&gt;How&lt;/em&gt; these scores get reported varies widely. Often, I see just the range of scores reported, hopefully with &lt;strong&gt;Cohen&amp;rsquo;s kappa&lt;/strong&gt; calculated in addition to the more straightforward &lt;strong&gt;percent agreement&lt;/strong&gt;. The kappa is important because it takes into account variance in the frequency of a code. For example, pecent agreement may be 97%, but if a code is used infrequently, kappa may be very low, even 0. Reporting kappa scores assures readers that in the instances when a rare code is used, the codebook is clear enough that a second coder will catch it.&lt;/p&gt;
&lt;p&gt;Of course, the whole qualitative coding process can be tricky, and in many cases, researchers must go through multiple rounds of dual coding a sample, revising the codebook, and coding a new sample. All of this nuance tends to get lost when researchers write their Method section, no doubt because this is an easy place to cut words to satisfy the requirements of a journal article.&lt;/p&gt;
&lt;p&gt;Reflecting on this whole process, I wanted to come up with a way to demonstrate the progress made through multiple rounds of dual coding for IRR. I came up with the plot displayed below for my dissertation study, which required five rounds of dual coding. Still, through this process, the IRR scores converged nicely in the fifth and final round. I&amp;rsquo;m pleased with this results as well as how this data visualization turned out.&lt;/p&gt;
&lt;h2 id=&#34;visualization-inter-rater-reliability-scores-plotted-across-five-rounds-of-dual-coding&#34;&gt;Visualization: Inter-rater Reliability Scores Plotted Across Five Rounds of Dual Coding&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://bretsw.com/img/irr-comparison-plot.png&#34; alt=&#34;Inter-rater reliability plot&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note.&lt;/em&gt; In my research, I follow a rule of thumb where two independent coders apply the codebook to 10% of the corpus in distinct rounds of coding until percent agreement on all codes is above 0.80 (i.e., 80%) and Cohenâ€™s kappa for all codes is above 0.60.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dissertation Themes</title>
      <link>http://bretsw.com/post/dissertation-themes/</link>
      <pubDate>Thu, 09 Jul 2020 00:00:00 +0000</pubDate>
      <guid>http://bretsw.com/post/dissertation-themes/</guid>
      <description>&lt;p&gt;As I&amp;rsquo;ve wrapped up numerous cycles of qualitative coding of interview data for my dissertation, &lt;em&gt;Into the edu-verse: New teachers seeking induction support on social media&lt;/em&gt;, I used &lt;code&gt;geom_tile()&lt;/code&gt; in &lt;strong&gt;{ggplot2}&lt;/strong&gt; to create heatmap plots to help me quickly visualize how thematic codes varied by interview participant.&lt;/p&gt;
&lt;h2 id=&#34;heatmap-comparison-of-thematic-codes-by-interview-participant&#34;&gt;Heatmap Comparison of Thematic Codes by Interview Participant&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://bretsw.com/img/dissertation-themes-wordcount-heatmap-sorted.png&#34; alt=&#34;Themes by participant&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note.&lt;/em&gt; Columns have been computationally reordered using principle components analysis (PCA) so that side-by-side columns are more similar than non-adjacent columns. Tile shading represents the number of words a participant spent speaking about a topic relative to how much they discussed other topics, rescaled to 0-100 for comparison between participants.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I&amp;rsquo;ve shared the code to produce these plots in a &lt;a href=&#34;https://github.com/bretsw/dissertation-themes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repository&lt;/a&gt;, and you can also &lt;a href=&#34;https://rpubs.com/bretsw/dissertation-themes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view on RPubs&lt;/a&gt; a variety of different heatmaps I created en route to these final plots.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
